{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment analysis"
      ],
      "metadata": {
        "id": "fw1dUITN9B1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install necessary libraries\n",
        "!pip install tensorflow\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Step 3: Load IMDB dataset for sentiment analysis (pre-built dataset in Keras)\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# Load dataset and limit the vocabulary to the top 10000 most frequent words\n",
        "vocab_size = 10000\n",
        "maxlen = 100  # Maximum length of reviews\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# Step 4: Pad sequences to ensure consistent input size for the model\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# Step 5: Split the training data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Build the sentiment analysis model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Embedding(vocab_size, 128, input_length=maxlen))\n",
        "model.add(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Step 7: Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Step 8: Train the model\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_val, y_val))\n",
        "\n",
        "# Step 9: Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Step 10: Predict sentiment on new data\n",
        "sample_text = \"This movie was amazing! I loved the characters and the plot was great.\"\n",
        "# Tokenize the input text (the tokenizer for IMDB is the same as the one used for training data)\n",
        "word_index = imdb.get_word_index()\n",
        "tokens = sample_text.lower().split()\n",
        "encoded = [word_index.get(word, 0) for word in tokens]  # Convert words to their index values\n",
        "encoded = pad_sequences([encoded], maxlen=maxlen)  # Pad the sequence\n",
        "\n",
        "# Predict sentiment\n",
        "prediction = model.predict(encoded)\n",
        "print(f\"Sentiment: {'Positive' if prediction >= 0.5 else 'Negative'}\")\n"
      ],
      "metadata": {
        "id": "pKf2oLrZ9CwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spelling correction"
      ],
      "metadata": {
        "id": "BFGUi9j89C4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install necessary libraries\n",
        "!pip install tensorflow\n",
        "!pip install nltk\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('words')\n",
        "\n",
        "# Step 3: Load and preprocess the dataset\n",
        "# We'll use a simple dataset of common misspellings and their correct words\n",
        "\n",
        "# Sample data (misspelled words -> correct words)\n",
        "corrections = [\n",
        "    (\"recieve\", \"receive\"),\n",
        "    (\"adn\", \"and\"),\n",
        "    (\"teh\", \"the\"),\n",
        "    (\"definately\", \"definitely\"),\n",
        "    (\"occured\", \"occurred\"),\n",
        "    (\"untill\", \"until\"),\n",
        "    (\"seperated\", \"separated\"),\n",
        "    (\"alot\", \"a lot\"),\n",
        "    (\"arguement\", \"argument\"),\n",
        "    (\"beleive\", \"believe\")\n",
        "]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "random.shuffle(corrections)\n",
        "train_data = corrections[:int(0.8*len(corrections))]\n",
        "test_data = corrections[int(0.8*len(corrections)):]\n",
        "\n",
        "# Step 4: Tokenize the words (characters) and prepare input-output sequences\n",
        "input_texts = [pair[0] for pair in train_data]\n",
        "target_texts = ['\\t' + pair[1] + '\\n' for pair in train_data]  # Add start and end token\n",
        "\n",
        "# Tokenization\n",
        "input_chars = sorted(set(''.join(input_texts)))\n",
        "target_chars = sorted(set(''.join(target_texts)))\n",
        "input_char_index = {char: i for i, char in enumerate(input_chars)}\n",
        "target_char_index = {char: i for i, char in enumerate(target_chars)}\n",
        "index_input_char = {i: char for char, i in input_char_index.items()}\n",
        "index_target_char = {i: char for char, i in target_char_index.items()}\n",
        "\n",
        "# Step 5: Vectorize the data (convert to integer sequences)\n",
        "max_input_length = max([len(txt) for txt in input_texts])\n",
        "max_target_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "encoder_input_data = np.zeros((len(input_texts), max_input_length), dtype='float32')\n",
        "decoder_input_data = np.zeros((len(input_texts), max_target_length), dtype='float32')\n",
        "decoder_target_data = np.zeros((len(input_texts), max_target_length, len(target_chars)), dtype='float32')\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t] = input_char_index[char]\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t] = target_char_index[char]\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_char_index[char]] = 1.\n",
        "\n",
        "# Step 6: Build the seq2seq model\n",
        "latent_dim = 256\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = layers.Input(shape=(None,))\n",
        "encoder_embedding = layers.Embedding(len(input_chars), latent_dim)(encoder_inputs)\n",
        "encoder_lstm = layers.LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = layers.Input(shape=(None,))\n",
        "decoder_embedding = layers.Embedding(len(target_chars), latent_dim)(decoder_inputs)\n",
        "decoder_lstm = layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = layers.Dense(len(target_chars), activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Model\n",
        "model = models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Step 7: Compile the model\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 8: Train the model\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=64, epochs=100, validation_split=0.2)\n",
        "\n",
        "# Step 9: Save the model\n",
        "model.save(\"spelling_correction_model.h5\")\n",
        "\n",
        "# Step 10: Evaluate the model (on test set)\n",
        "input_texts_test = [pair[0] for pair in test_data]\n",
        "target_texts_test = ['\\t' + pair[1] + '\\n' for pair in test_data]\n",
        "encoder_input_data_test = np.zeros((len(input_texts_test), max_input_length), dtype='float32')\n",
        "decoder_input_data_test = np.zeros((len(input_texts_test), max_target_length), dtype='float32')\n",
        "decoder_target_data_test = np.zeros((len(input_texts_test), max_target_length, len(target_chars)), dtype='float32')\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts_test, target_texts_test)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data_test[i, t] = input_char_index[char]\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data_test[i, t] = target_char_index[char]\n",
        "        if t > 0:\n",
        "            decoder_target_data_test[i, t - 1, target_char_index[char]] = 1.\n",
        "\n",
        "test_loss, test_acc = model.evaluate([encoder_input_data_test, decoder_input_data_test], decoder_target_data_test)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Step 11: Predict spelling correction on new data\n",
        "def predict_spelling_correction(input_text):\n",
        "    encoder_input_seq = np.zeros((1, max_input_length), dtype='float32')\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_seq[0, t] = input_char_index.get(char, 0)\n",
        "\n",
        "    states_value = model.predict(encoder_input_seq)\n",
        "\n",
        "    # Prepare the decoder input\n",
        "    target_seq = np.zeros((1, 1), dtype='float32')\n",
        "    target_seq[0, 0] = target_char_index['\\t']\n",
        "\n",
        "    # Generate the output sequence\n",
        "    decoded_sentence = ''\n",
        "    for _ in range(max_target_length):\n",
        "        output_tokens = model.predict([encoder_input_seq, target_seq])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = index_target_char[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        if sampled_char == '\\n':\n",
        "            break\n",
        "\n",
        "        # Update the target sequence\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "    return decoded_sentence.strip()\n",
        "\n",
        "# Example usage\n",
        "input_word = \"recieve\"\n",
        "corrected_word = predict_spelling_correction(input_word)\n",
        "print(f\"Corrected Word: {corrected_word}\")\n"
      ],
      "metadata": {
        "id": "Dtlja8639C-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word sense disambiguation"
      ],
      "metadata": {
        "id": "zq_6Z0PV9DHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install necessary libraries\n",
        "!pip install tensorflow\n",
        "!pip install nltk\n",
        "!pip install gensim\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('wordnet')\n",
        "nltk.download('semcor')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Step 3: Load SemCor dataset for word sense disambiguation (WSD)\n",
        "from nltk.corpus import semcor\n",
        "\n",
        "# Sample sentence from SemCor (context with sense-tagged word)\n",
        "context = semcor.sents()[:1000]  # Use first 1000 sentences for training\n",
        "\n",
        "# Step 4: Preprocessing and extracting word-context pairs\n",
        "word_contexts = []\n",
        "sense_labels = []\n",
        "\n",
        "for sentence in context:\n",
        "    for word in sentence:\n",
        "        if isinstance(word, tuple):  # Word with sense tag (e.g., ('dog', 'n'))\n",
        "            word_token, sense_tag = word\n",
        "            word_contexts.append(sentence)\n",
        "            sense_labels.append(sense_tag)\n",
        "\n",
        "# Step 5: Prepare word embeddings using Word2Vec for context representation\n",
        "# Using Word2Vec to create word embeddings for each word in the context\n",
        "model_w2v = Word2Vec(sentences=[nltk.word_tokenize(' '.join(sent)) for sent in context], vector_size=100, window=5, min_count=1, sg=0)\n",
        "\n",
        "# Step 6: Convert word-context pairs into feature vectors\n",
        "def get_word_embedding(word):\n",
        "    try:\n",
        "        return model_w2v.wv[word]\n",
        "    except KeyError:\n",
        "        return np.zeros(100)\n",
        "\n",
        "X = np.array([np.mean([get_word_embedding(word) for word in sent if isinstance(word, str)], axis=0) for sent in word_contexts])\n",
        "y = np.array([wn.synset(sense).lemmas()[0].key() for sense in sense_labels])\n",
        "\n",
        "# Step 7: Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 8: Build a simple Neural Network model for WSD\n",
        "model = models.Sequential()\n",
        "model.add(layers.Input(shape=(100,)))  # Input layer (word embeddings)\n",
        "model.add(layers.Dense(128, activation='relu'))  # Hidden layer\n",
        "model.add(layers.Dense(len(set(y)), activation='softmax'))  # Output layer (classification)\n",
        "\n",
        "# Step 9: Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 10: Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n",
        "\n",
        "# Step 11: Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Step 12: Predict the word sense for new context\n",
        "def predict_word_sense(context_sentence):\n",
        "    sentence_embedding = np.mean([get_word_embedding(word) for word in nltk.word_tokenize(context_sentence)], axis=0).reshape(1, -1)\n",
        "    prediction = model.predict(sentence_embedding)\n",
        "    sense_index = np.argmax(prediction)\n",
        "    return wn.lemmas()[sense_index].name()  # Get the sense name from WordNet\n",
        "\n",
        "# Example usage: Predict the word sense for a new sentence\n",
        "context_sentence = \"The bank was full of fish.\"\n",
        "predicted_sense = predict_word_sense(context_sentence)\n",
        "print(f\"Predicted Sense: {predicted_sense}\")\n"
      ],
      "metadata": {
        "id": "NPaPSU-L9DNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine translation"
      ],
      "metadata": {
        "id": "MDFsE2Y99DTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install necessary libraries\n",
        "!pip install tensorflow\n",
        "!pip install nltk\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.translate import bleu_score\n",
        "\n",
        "# Step 3: Download and preprocess data (English-French translation pairs)\n",
        "nltk.download('translate')\n",
        "\n",
        "# Example parallel corpus (English-French translation)\n",
        "english_sentences = [\n",
        "    'hello', 'how are you', 'good morning', 'good evening', 'good night',\n",
        "    'thank you', 'goodbye', 'please', 'sorry', 'excuse me'\n",
        "]\n",
        "\n",
        "french_sentences = [\n",
        "    'bonjour', 'comment ça va', 'bon matin', 'bonsoir', 'bonne nuit',\n",
        "    'merci', 'au revoir', 's’il vous plaît', 'désolé', 'excusez-moi'\n",
        "]\n",
        "\n",
        "# Step 4: Tokenization and creating word indices\n",
        "input_texts = english_sentences\n",
        "output_texts = ['\\t' + sentence + '\\n' for sentence in french_sentences]  # Adding start and end token\n",
        "\n",
        "input_chars = sorted(set(''.join(input_texts)))\n",
        "output_chars = sorted(set(''.join(output_texts)))\n",
        "input_char_index = {char: i for i, char in enumerate(input_chars)}\n",
        "output_char_index = {char: i for i, char in enumerate(output_chars)}\n",
        "index_input_char = {i: char for char, i in input_char_index.items()}\n",
        "index_output_char = {i: char for char, i in output_char_index.items()}\n",
        "\n",
        "# Step 5: Vectorizing the data (convert to integer sequences)\n",
        "max_input_length = max([len(txt) for txt in input_texts])\n",
        "max_output_length = max([len(txt) for txt in output_texts])\n",
        "\n",
        "encoder_input_data = np.zeros((len(input_texts), max_input_length), dtype='float32')\n",
        "decoder_input_data = np.zeros((len(input_texts), max_output_length), dtype='float32')\n",
        "decoder_target_data = np.zeros((len(input_texts), max_output_length, len(output_chars)), dtype='float32')\n",
        "\n",
        "for i, (input_text, output_text) in enumerate(zip(input_texts, output_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t] = input_char_index[char]\n",
        "    for t, char in enumerate(output_text):\n",
        "        decoder_input_data[i, t] = output_char_index[char]\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, output_char_index[char]] = 1.\n",
        "\n",
        "# Step 6: Build Seq2Seq model with attention mechanism\n",
        "latent_dim = 256\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = layers.Input(shape=(None,))\n",
        "encoder_embedding = layers.Embedding(len(input_chars), latent_dim)(encoder_inputs)\n",
        "encoder_lstm = layers.LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = layers.Input(shape=(None,))\n",
        "decoder_embedding = layers.Embedding(len(output_chars), latent_dim)(decoder_inputs)\n",
        "decoder_lstm = layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = layers.Dense(len(output_chars), activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Model\n",
        "model = models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Step 7: Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 8: Train the model\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=64, epochs=100, validation_split=0.2)\n",
        "\n",
        "# Step 9: Evaluate the model (on test set)\n",
        "test_loss, test_acc = model.evaluate([encoder_input_data, decoder_input_data], decoder_target_data)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Step 10: Predict translation on new data\n",
        "def predict_translation(input_text):\n",
        "    encoder_input_seq = np.zeros((1, max_input_length), dtype='float32')\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_seq[0, t] = input_char_index.get(char, 0)\n",
        "\n",
        "    states_value = model.predict(encoder_input_seq)\n",
        "\n",
        "    # Prepare the decoder input\n",
        "    target_seq = np.zeros((1, 1), dtype='float32')\n",
        "    target_seq[0, 0] = output_char_index['\\t']\n",
        "\n",
        "    # Generate the output sequence\n",
        "    decoded_sentence = ''\n",
        "    for _ in range(max_output_length):\n",
        "        output_tokens = model.predict([encoder_input_seq, target_seq])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = index_output_char[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        if sampled_char == '\\n':\n",
        "            break\n",
        "\n",
        "        # Update the target sequence\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "    return decoded_sentence.strip()\n",
        "\n",
        "# Example usage: Translate a new sentence from English to French\n",
        "input_sentence = \"hello\"\n",
        "predicted_translation = predict_translation(input_sentence)\n",
        "print(f\"Predicted Translation: {predicted_translation}\")\n"
      ],
      "metadata": {
        "id": "fvaMPmzK9Dbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Classification"
      ],
      "metadata": {
        "id": "S2CQKMIJ9Dh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install necessary libraries\n",
        "!pip install tensorflow\n",
        "!pip install nltk\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import nltk\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# Step 3: Load the IMDB dataset\n",
        "# IMDB dataset is a built-in dataset in TensorFlow/Keras (reviews and sentiment labels)\n",
        "num_words = 10000  # Limit to top 10,000 most frequent words\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "# Step 4: Pad sequences to ensure uniform input length\n",
        "max_len = 500  # Maximum sequence length (we'll truncate/pad sequences to this length)\n",
        "x_train_pad = pad_sequences(x_train, maxlen=max_len, padding='post', truncating='post')\n",
        "x_test_pad = pad_sequences(x_test, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "# Step 5: Build the Text Classification Model (LSTM-based)\n",
        "model = models.Sequential()\n",
        "\n",
        "# Embedding layer to learn word representations\n",
        "model.add(layers.Embedding(input_dim=num_words, output_dim=128, input_length=max_len))\n",
        "\n",
        "# LSTM layer for sequence processing\n",
        "model.add(layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "# Dense layer for classification (output is binary: positive/negative)\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Step 6: Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 7: Train the model\n",
        "model.fit(x_train_pad, y_train, epochs=5, batch_size=64, validation_data=(x_test_pad, y_test))\n",
        "\n",
        "# Step 8: Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test_pad, y_test)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Step 9: Predict sentiment of a new review\n",
        "def predict_sentiment(review):\n",
        "    # Tokenize and pad the review text\n",
        "    review_seq = imdb.get_word_index()\n",
        "    review_tokenized = [review_seq.get(word, 0) for word in review.split()]\n",
        "    review_padded = pad_sequences([review_tokenized], maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "    # Predict sentiment (0 for negative, 1 for positive)\n",
        "    sentiment = model.predict(review_padded)\n",
        "    return \"Positive\" if sentiment >= 0.5 else \"Negative\"\n",
        "\n",
        "# Example usage: Predict sentiment for a new review\n",
        "review_text = \"This movie was fantastic! I really enjoyed it.\"\n",
        "predicted_sentiment = predict_sentiment(review_text)\n",
        "print(f\"Predicted Sentiment: {predicted_sentiment}\")\n"
      ],
      "metadata": {
        "id": "sZz9v8bR9Do9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question answering system."
      ],
      "metadata": {
        "id": "RZFexPg39DvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install necessary libraries\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install datasets\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "from transformers import pipeline, BertTokenizer, BertForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# Step 3: Load the pre-trained BERT model and tokenizer for Question Answering\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "# Step 4: Create the Question Answering pipeline\n",
        "qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Step 5: Define context (passage) and a sample question\n",
        "context = \"\"\"\n",
        "Bert (Bidirectional Encoder Representations from Transformers) is a transformer-based model designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. The model has achieved state-of-the-art results on a wide array of NLP tasks, including Question Answering (QA), Language Inference (LI), and Named Entity Recognition (NER). The BERT model is trained using large corpora and fine-tuned on smaller datasets to adapt to specific tasks.\n",
        "\"\"\"\n",
        "\n",
        "question = \"What tasks has BERT achieved state-of-the-art results on?\"\n",
        "\n",
        "# Step 6: Use the model to answer the question\n",
        "result = qa_pipeline({'context': context, 'question': question})\n",
        "\n",
        "# Step 7: Output the result (answer)\n",
        "print(f\"Answer: {result['answer']}\")\n"
      ],
      "metadata": {
        "id": "tov0INtt9D1l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}