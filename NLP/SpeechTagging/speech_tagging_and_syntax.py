# -*- coding: utf-8 -*-
"""Speech Tagging and Syntax.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LvLwjIyBm_yhwYZf2a1WwjLctYId2Qmz

Stochastic POS tagging
"""

import nltk
from nltk.tag import hmm
from nltk.corpus import treebank

# Download required NLTK data
nltk.download('treebank')
nltk.download('punkt')

# Load the treebank corpus
train_data = treebank.tagged_sents()[:3000]  # Training data
test_data = treebank.tagged_sents()[3000:]  # Testing data

# Train a Hidden Markov Model (HMM) tagger
trainer = hmm.HiddenMarkovModelTrainer()
hmm_tagger = trainer.train(train_data)

# Evaluate the tagger
accuracy = hmm_tagger.evaluate(test_data)
print(f"Accuracy of Stochastic (HMM) POS Tagger: {accuracy:.2%}")

# Example tagging with the HMM tagger
sentence = nltk.word_tokenize("Technology evolves at an unprecedented pace.")
tagged_sentence = hmm_tagger.tag(sentence)
print("Tagged Sentence:", tagged_sentence)

"""HMM"""

import nltk
from nltk.tag import hmm
from nltk.corpus import treebank

# Download required NLTK data
nltk.download('treebank')
nltk.download('punkt')

# Load the treebank corpus
train_data = treebank.tagged_sents()[:3000]  # Training data
test_data = treebank.tagged_sents()[3000:]  # Testing data

# Train a Hidden Markov Model (HMM) tagger
trainer = hmm.HiddenMarkovModelTrainer()
hmm_tagger = trainer.train(train_data)

# Evaluate the HMM tagger
accuracy = hmm_tagger.evaluate(test_data)
print(f"Accuracy of HMM Tagger: {accuracy:.2%}")

# Example tagging with the HMM tagger
sentence = nltk.word_tokenize("Artificial intelligence is transforming industries worldwide.")
tagged_sentence = hmm_tagger.tag(sentence)
print("Tagged Sentence:", tagged_sentence)

"""Transformation based tagging (TBL)"""

import nltk
from nltk.tag import brill, brill_trainer
from nltk.corpus import treebank

# Download required NLTK data
nltk.download('treebank')
nltk.download('punkt')

# Load the treebank corpus
train_data = treebank.tagged_sents()[:3000]  # Training data
test_data = treebank.tagged_sents()[3000:]  # Testing data

# Define a baseline tagger (e.g., UnigramTagger)
baseline_tagger = nltk.UnigramTagger(train_data)

# Define the templates for transformation rules
templates = brill.nltkdemo18()  # Predefined templates in NLTK

# Train a Brill tagger
trainer = brill_trainer.BrillTaggerTrainer(baseline_tagger, templates)
brill_tagger = trainer.train(train_data, max_rules=50)  # Adjust max_rules as needed

# Evaluate the tagger
accuracy = brill_tagger.evaluate(test_data)
print(f"Accuracy of Brill Tagger: {accuracy:.2%}")

# Example tagging with the Brill tagger
sentence = nltk.word_tokenize("The quick brown fox jumps over the lazy dog.")
tagged_sentence = brill_tagger.tag(sentence)
print("Tagged Sentence:", tagged_sentence)

"""Handling of unknown words"""

import nltk
from nltk.tag import UnigramTagger, DefaultTagger, RegexpTagger
from nltk.corpus import treebank

# Download required NLTK data
nltk.download('treebank')
nltk.download('punkt')

# Load the treebank corpus
train_data = treebank.tagged_sents()[:3000]  # Training data
test_data = treebank.tagged_sents()[3000:]  # Testing data

# Define a default tagger for unknown words
default_tagger = DefaultTagger('NN')  # Tags unknown words as nouns

# Define a regex-based tagger for specific patterns
regex_tagger = RegexpTagger(
    [
        (r'^-?[0-9]+(\.[0-9]+)?$', 'CD'),  # Numbers as cardinal digits
        (r'(The|the|A|a|An|an)$', 'DT'),  # Common determiners
        (r'.*ing$', 'VBG'),               # Gerunds
        (r'.*ed$', 'VBD'),                # Past tense verbs
        (r'.*es$', 'VBZ'),                # Verbs ending in 'es'
        (r'.*ly$', 'RB'),                 # Adverbs
        (r'.*able$', 'JJ'),               # Adjectives
        (r'.*', 'NN')                     # Default to nouns
    ],
    backoff=default_tagger
)

# Define a UnigramTagger with a backoff to handle unknown words
unigram_tagger = UnigramTagger(train_data, backoff=regex_tagger)

# Evaluate the tagger
accuracy = unigram_tagger.evaluate(test_data)
print(f"Accuracy of Tagger with Unknown Word Handling: {accuracy:.2%}")

# Example tagging with the tagger
sentence = nltk.word_tokenize("Innovative solutions emerge rapidly, fueled by data-driven strategies.")
tagged_sentence = unigram_tagger.tag(sentence)
print("Tagged Sentence:", tagged_sentence)

"""Named entities"""

import nltk
from nltk import word_tokenize, pos_tag, ne_chunk

# Download required NLTK data
nltk.download('punkt')
nltk.download('maxent_ne_chunker')
nltk.download('words')

# Define a sample sentence
sentence = "Barack Obama was born in Hawaii and served as the President of the United States."

# Tokenize and POS-tag the sentence
tokens = word_tokenize(sentence)
pos_tags = pos_tag(tokens)

# Perform Named Entity Recognition (NER)
named_entities = ne_chunk(pos_tags)
print("Named Entities:")
print(named_entities)

"""Multi word expressions"""

import nltk
from nltk.collocations import BigramCollocationFinder
from nltk.metrics import BigramAssocMeasures
from nltk.tokenize import word_tokenize

# Sample text
text = """
Natural language processing involves tasks like tokenization, part-of-speech tagging,
and multi-word expression identification. Machine learning techniques help identify common patterns.
"""

# Tokenize the text
tokens = word_tokenize(text)

# Find bigram collocations (two-word MWEs)
bigram_finder = BigramCollocationFinder.from_words(tokens)

# Score bigrams using Pointwise Mutual Information (PMI)
scored_bigrams = bigram_finder.score_ngrams(BigramAssocMeasures.pmi)

# Filter and print the top MWEs
top_mwes = sorted(scored_bigrams, key=lambda x: -x[1])[:10]  # Top 10 MWEs
print("Top Multi-Word Expressions:")
for bigram, score in top_mwes:
    print(f"{bigram} (Score: {score:.2f})")

"""Speech Processing: Speech
and phonetics, Vocal organ, Phonological rules,
"""

import speech_recognition as sr
import nltk
from nltk.corpus import cmudict

# Download NLTK's CMU Pronouncing Dictionary
nltk.download('cmudict')

# Initialize the CMU Pronouncing Dictionary
pronouncing_dict = cmudict.dict()

# Recognize speech from audio (SpeechRecognition library)
def recognize_speech(audio_file):
    recognizer = sr.Recognizer()
    with sr.AudioFile(audio_file) as source:
        audio = recognizer.record(source)
    try:
        text = recognizer.recognize_google(audio)
        return text
    except sr.UnknownValueError:
        return "Speech not recognized"
    except sr.RequestError:
        return "Service unavailable"

# Analyze phonetics using the CMU Pronouncing Dictionary
def analyze_phonetics(word):
    return pronouncing_dict.get(word.lower(), ["Phonetics not found"])

# Example usage
audio_file_path = "example.wav"  # Replace with your audio file path
recognized_text = recognize_speech(audio_file_path)
print("Recognized Text:", recognized_text)

# Phonetic and phonological analysis
words = nltk.word_tokenize(recognized_text)
for word in words:
    phonetics = analyze_phonetics(word)
    print(f"Word: {word}, Phonetics: {phonetics}")

"""Probabilistic models-Spelling error"""

from symspellpy import SymSpell, Verbosity
import nltk

# Initialize SymSpell with a dictionary
sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)

# Load a pre-built dictionary file (or create one)
dictionary_path = nltk.download('words')  # Use NLTK's words corpus as a source
sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1, separator=",")

# Function to correct spelling errors probabilistically
def correct_spelling(word):
    suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)
    if suggestions:
        return suggestions[0].term  # Return the most likely correction
    return word  # Return the original word if no correction is found

# Example usage
sentence = "Speling erors are commn in writen text."
tokens = nltk.word_tokenize(sentence)
corrected_sentence = " ".join(correct_spelling(word) for word in tokens)

print("Original Sentence:", sentence)
print("Corrected Sentence:", corrected_sentence)

"""Bayesian method to spelling"""

import re
from collections import Counter

# Load and preprocess a corpus to build a word frequency model
def load_corpus(file_path):
    with open(file_path, 'r') as f:
        text = f.read().lower()
    return Counter(re.findall(r'\w+', text))

# Calculate probabilities using Bayes' theorem
def bayesian_correct(word, word_counts, alphabet="abcdefghijklmnopqrstuvwxyz"):
    def edits1(w):
        """All possible edits at an edit distance of 1"""
        splits = [(w[:i], w[i:]) for i in range(len(w) + 1)]
        deletes = [L + R[1:] for L, R in splits if R]
        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]
        replaces = [L + c + R[1:] for L, R in splits if R for c in alphabet]
        inserts = [L + c + R for L, R in splits for c in alphabet]
        return set(deletes + transposes + replaces + inserts)

    def known(words):
        """Filter words that exist in the dictionary"""
        return {w for w in words if w in word_counts}

    def candidates(w):
        """Generate possible corrections"""
        return (known([w]) or known(edits1(w)) or [w])

    def probability(w):
        """Calculate word probability"""
        return word_counts[w] / sum(word_counts.values())

    return max(candidates(word), key=probability)

# Load a sample corpus and test the Bayesian spelling corrector
word_counts = load_corpus('sample_corpus.txt')  # Replace with your corpus file
misspelled_word = "speling"
corrected_word = bayesian_correct(misspelled_word, word_counts)

print(f"Misspelled Word: {misspelled_word}")
print(f"Corrected Word: {corrected_word}")

"""Minimum edit distance"""

def min_edit_distance(source, target):
    m, n = len(source), len(target)
    dp = [[0 for _ in range(n + 1)] for _ in range(m + 1)]

    # Initialize the DP table
    for i in range(m + 1):
        dp[i][0] = i  # Cost of deletions
    for j in range(n + 1):
        dp[0][j] = j  # Cost of insertions

    # Compute minimum edit distance
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if source[i - 1] == target[j - 1]:  # No cost if characters match
                dp[i][j] = dp[i - 1][j - 1]
            else:
                dp[i][j] = 1 + min(
                    dp[i - 1][j],    # Deletion
                    dp[i][j - 1],    # Insertion
                    dp[i - 1][j - 1] # Substitution
                )

    return dp[m][n]

# Example usage
source_word = "intention"
target_word = "execution"
distance = min_edit_distance(source_word, target_word)

print(f"Minimum Edit Distance between '{source_word}' and '{target_word}': {distance}")

"""Bayesian method of pronunciation variation"""

import nltk
from nltk.corpus import cmudict
from collections import Counter

# Load the CMU Pronouncing Dictionary
nltk.download('cmudict')
pronouncing_dict = cmudict.dict()

# Example phonetic variation probabilities (simplified for demonstration)
phonetic_variation_prob = {
    "AE": {"AA": 0.2, "AE": 0.8},
    "T": {"D": 0.1, "T": 0.9},
}

# Bayesian method for pronunciation variation
def bayesian_pronunciation(word):
    if word.lower() not in pronouncing_dict:
        return f"No pronunciation data available for '{word}'"

    pronunciations = pronouncing_dict[word.lower()]  # All known pronunciations
    probabilities = []

    for pronunciation in pronunciations:
        prob = 1.0
        for phoneme in pronunciation:
            if phoneme in phonetic_variation_prob:
                prob *= phonetic_variation_prob[phoneme].get(phoneme, 0.1)
            else:
                prob *= 1.0  # Assume full probability for unchanged phonemes
        probabilities.append((tuple(pronunciation), prob))

    # Select the most likely pronunciation
    best_pronunciation = max(probabilities, key=lambda x: x[1])
    return best_pronunciation

# Example usage
word = "data"
result = bayesian_pronunciation(word)
print(f"Most likely pronunciation for '{word}': {result[0]} (Probability: {result[1]:.2f})")

"""Viterbi algorithm"""

import nltk
from nltk.corpus import treebank

# Load a tagged dataset
nltk.download('treebank')
tagged_sents = treebank.tagged_sents()[:3000]

# Train HMM parameters (simplified)
def train_hmm(tagged_sents):
    tags = []
    words = []
    for sent in tagged_sents:
        tags.append("START")  # Add a START tag for the beginning
        for word, tag in sent:
            words.append(word)
            tags.append(tag)
        tags.append("END")  # Add an END tag at the end

    tag_freq = nltk.ConditionalFreqDist(nltk.bigrams(tags))
    word_tag_freq = nltk.ConditionalFreqDist(((tag, word) for sent in tagged_sents for word, tag in sent))
    return tag_freq, word_tag_freq

tag_freq, word_tag_freq = train_hmm(tagged_sents)

# Viterbi Algorithm
def viterbi(sentence, tag_freq, word_tag_freq):
    words = nltk.word_tokenize(sentence)
    states = set(tag for tag, _ in tag_freq.items())
    V = [{}]
    path = {}

    # Initialize base cases (t = 0)
    for tag in states:
        V[0][tag] = tag_freq["START"].freq(tag) * word_tag_freq[tag].freq(words[0])
        path[tag] = [tag]

    # Run Viterbi for t > 0
    for t in range(1, len(words)):
        V.append({})
        new_path = {}
        for tag in states:
            (prob, state) = max(
                (V[t-1][y0] * tag_freq[y0].freq(tag) * word_tag_freq[tag].freq(words[t]), y0)
                for y0 in states if V[t-1][y0] > 0
            )
            V[t][tag] = prob
            new_path[tag] = path[state] + [tag]
        path = new_path

    # Find the most probable state sequence
    (prob, state) = max((V[-1][y], y) for y in states)
    return prob, path[state]

# Example usage
sentence = "The quick brown fox jumps over the lazy dog."
probability, best_path = viterbi(sentence, tag_freq, word_tag_freq)
print(f"Most likely tags: {best_path}")
print(f"Sequence Probability: {probability}")

"""Speech recognition."""

import speech_recognition as sr

# Initialize recognizer
recognizer = sr.Recognizer()

# Function to recognize speech from an audio file
def recognize_speech_from_audio(audio_file):
    with sr.AudioFile(audio_file) as source:
        audio_data = recognizer.record(source)
    try:
        # Use Google Web Speech API for speech recognition
        recognized_text = recognizer.recognize_google(audio_data)
        return recognized_text
    except sr.UnknownValueError:
        return "Sorry, I could not understand the audio."
    except sr.RequestError:
        return "Could not request results from the speech recognition service."

# Example usage
audio_file = "example_audio.wav"  # Replace with the path to your audio file
recognized_text = recognize_speech_from_audio(audio_file)

print(f"Recognized Speech: {recognized_text}")