{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic POS tagging"
      ],
      "metadata": {
        "id": "zoR7VJ7R6PRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import hmm\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('treebank')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the treebank corpus\n",
        "train_data = treebank.tagged_sents()[:3000]  # Training data\n",
        "test_data = treebank.tagged_sents()[3000:]  # Testing data\n",
        "\n",
        "# Train a Hidden Markov Model (HMM) tagger\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "hmm_tagger = trainer.train(train_data)\n",
        "\n",
        "# Evaluate the tagger\n",
        "accuracy = hmm_tagger.evaluate(test_data)\n",
        "print(f\"Accuracy of Stochastic (HMM) POS Tagger: {accuracy:.2%}\")\n",
        "\n",
        "# Example tagging with the HMM tagger\n",
        "sentence = nltk.word_tokenize(\"Technology evolves at an unprecedented pace.\")\n",
        "tagged_sentence = hmm_tagger.tag(sentence)\n",
        "print(\"Tagged Sentence:\", tagged_sentence)\n"
      ],
      "metadata": {
        "id": "1ilvahm66Q62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HMM"
      ],
      "metadata": {
        "id": "cn70_xns6h6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import hmm\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('treebank')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the treebank corpus\n",
        "train_data = treebank.tagged_sents()[:3000]  # Training data\n",
        "test_data = treebank.tagged_sents()[3000:]  # Testing data\n",
        "\n",
        "# Train a Hidden Markov Model (HMM) tagger\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "hmm_tagger = trainer.train(train_data)\n",
        "\n",
        "# Evaluate the HMM tagger\n",
        "accuracy = hmm_tagger.evaluate(test_data)\n",
        "print(f\"Accuracy of HMM Tagger: {accuracy:.2%}\")\n",
        "\n",
        "# Example tagging with the HMM tagger\n",
        "sentence = nltk.word_tokenize(\"Artificial intelligence is transforming industries worldwide.\")\n",
        "tagged_sentence = hmm_tagger.tag(sentence)\n",
        "print(\"Tagged Sentence:\", tagged_sentence)\n"
      ],
      "metadata": {
        "id": "W15TUPt96hC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformation based tagging (TBL)"
      ],
      "metadata": {
        "id": "QVSX26iY6ca9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import brill, brill_trainer\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('treebank')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the treebank corpus\n",
        "train_data = treebank.tagged_sents()[:3000]  # Training data\n",
        "test_data = treebank.tagged_sents()[3000:]  # Testing data\n",
        "\n",
        "# Define a baseline tagger (e.g., UnigramTagger)\n",
        "baseline_tagger = nltk.UnigramTagger(train_data)\n",
        "\n",
        "# Define the templates for transformation rules\n",
        "templates = brill.nltkdemo18()  # Predefined templates in NLTK\n",
        "\n",
        "# Train a Brill tagger\n",
        "trainer = brill_trainer.BrillTaggerTrainer(baseline_tagger, templates)\n",
        "brill_tagger = trainer.train(train_data, max_rules=50)  # Adjust max_rules as needed\n",
        "\n",
        "# Evaluate the tagger\n",
        "accuracy = brill_tagger.evaluate(test_data)\n",
        "print(f\"Accuracy of Brill Tagger: {accuracy:.2%}\")\n",
        "\n",
        "# Example tagging with the Brill tagger\n",
        "sentence = nltk.word_tokenize(\"The quick brown fox jumps over the lazy dog.\")\n",
        "tagged_sentence = brill_tagger.tag(sentence)\n",
        "print(\"Tagged Sentence:\", tagged_sentence)\n"
      ],
      "metadata": {
        "id": "6lM0IfhB6c0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling of unknown words"
      ],
      "metadata": {
        "id": "ncL73Htt6WeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import UnigramTagger, DefaultTagger, RegexpTagger\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('treebank')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the treebank corpus\n",
        "train_data = treebank.tagged_sents()[:3000]  # Training data\n",
        "test_data = treebank.tagged_sents()[3000:]  # Testing data\n",
        "\n",
        "# Define a default tagger for unknown words\n",
        "default_tagger = DefaultTagger('NN')  # Tags unknown words as nouns\n",
        "\n",
        "# Define a regex-based tagger for specific patterns\n",
        "regex_tagger = RegexpTagger(\n",
        "    [\n",
        "        (r'^-?[0-9]+(\\.[0-9]+)?$', 'CD'),  # Numbers as cardinal digits\n",
        "        (r'(The|the|A|a|An|an)$', 'DT'),  # Common determiners\n",
        "        (r'.*ing$', 'VBG'),               # Gerunds\n",
        "        (r'.*ed$', 'VBD'),                # Past tense verbs\n",
        "        (r'.*es$', 'VBZ'),                # Verbs ending in 'es'\n",
        "        (r'.*ly$', 'RB'),                 # Adverbs\n",
        "        (r'.*able$', 'JJ'),               # Adjectives\n",
        "        (r'.*', 'NN')                     # Default to nouns\n",
        "    ],\n",
        "    backoff=default_tagger\n",
        ")\n",
        "\n",
        "# Define a UnigramTagger with a backoff to handle unknown words\n",
        "unigram_tagger = UnigramTagger(train_data, backoff=regex_tagger)\n",
        "\n",
        "# Evaluate the tagger\n",
        "accuracy = unigram_tagger.evaluate(test_data)\n",
        "print(f\"Accuracy of Tagger with Unknown Word Handling: {accuracy:.2%}\")\n",
        "\n",
        "# Example tagging with the tagger\n",
        "sentence = nltk.word_tokenize(\"Innovative solutions emerge rapidly, fueled by data-driven strategies.\")\n",
        "tagged_sentence = unigram_tagger.tag(sentence)\n",
        "print(\"Tagged Sentence:\", tagged_sentence)\n"
      ],
      "metadata": {
        "id": "oKvgb5A06XB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named entities"
      ],
      "metadata": {
        "id": "FQQtaJ7q6ue1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Define a sample sentence\n",
        "sentence = \"Barack Obama was born in Hawaii and served as the President of the United States.\"\n",
        "\n",
        "# Tokenize and POS-tag the sentence\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "# Perform Named Entity Recognition (NER)\n",
        "named_entities = ne_chunk(pos_tags)\n",
        "print(\"Named Entities:\")\n",
        "print(named_entities)\n"
      ],
      "metadata": {
        "id": "Pw72c9YZ6ust"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi word expressions"
      ],
      "metadata": {
        "id": "6CxmkBdt6u79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample text\n",
        "text = \"\"\"\n",
        "Natural language processing involves tasks like tokenization, part-of-speech tagging,\n",
        "and multi-word expression identification. Machine learning techniques help identify common patterns.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Find bigram collocations (two-word MWEs)\n",
        "bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
        "\n",
        "# Score bigrams using Pointwise Mutual Information (PMI)\n",
        "scored_bigrams = bigram_finder.score_ngrams(BigramAssocMeasures.pmi)\n",
        "\n",
        "# Filter and print the top MWEs\n",
        "top_mwes = sorted(scored_bigrams, key=lambda x: -x[1])[:10]  # Top 10 MWEs\n",
        "print(\"Top Multi-Word Expressions:\")\n",
        "for bigram, score in top_mwes:\n",
        "    print(f\"{bigram} (Score: {score:.2f})\")\n"
      ],
      "metadata": {
        "id": "Bb-mXRUX6vGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Speech Processing: Speech\n",
        "and phonetics, Vocal organ, Phonological rules,"
      ],
      "metadata": {
        "id": "ekUDsZEE6vRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr\n",
        "import nltk\n",
        "from nltk.corpus import cmudict\n",
        "\n",
        "# Download NLTK's CMU Pronouncing Dictionary\n",
        "nltk.download('cmudict')\n",
        "\n",
        "# Initialize the CMU Pronouncing Dictionary\n",
        "pronouncing_dict = cmudict.dict()\n",
        "\n",
        "# Recognize speech from audio (SpeechRecognition library)\n",
        "def recognize_speech(audio_file):\n",
        "    recognizer = sr.Recognizer()\n",
        "    with sr.AudioFile(audio_file) as source:\n",
        "        audio = recognizer.record(source)\n",
        "    try:\n",
        "        text = recognizer.recognize_google(audio)\n",
        "        return text\n",
        "    except sr.UnknownValueError:\n",
        "        return \"Speech not recognized\"\n",
        "    except sr.RequestError:\n",
        "        return \"Service unavailable\"\n",
        "\n",
        "# Analyze phonetics using the CMU Pronouncing Dictionary\n",
        "def analyze_phonetics(word):\n",
        "    return pronouncing_dict.get(word.lower(), [\"Phonetics not found\"])\n",
        "\n",
        "# Example usage\n",
        "audio_file_path = \"example.wav\"  # Replace with your audio file path\n",
        "recognized_text = recognize_speech(audio_file_path)\n",
        "print(\"Recognized Text:\", recognized_text)\n",
        "\n",
        "# Phonetic and phonological analysis\n",
        "words = nltk.word_tokenize(recognized_text)\n",
        "for word in words:\n",
        "    phonetics = analyze_phonetics(word)\n",
        "    print(f\"Word: {word}, Phonetics: {phonetics}\")\n"
      ],
      "metadata": {
        "id": "6DOUx8q66vZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probabilistic models-Spelling error"
      ],
      "metadata": {
        "id": "lyTuuHKc6vgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from symspellpy import SymSpell, Verbosity\n",
        "import nltk\n",
        "\n",
        "# Initialize SymSpell with a dictionary\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "\n",
        "# Load a pre-built dictionary file (or create one)\n",
        "dictionary_path = nltk.download('words')  # Use NLTK's words corpus as a source\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1, separator=\",\")\n",
        "\n",
        "# Function to correct spelling errors probabilistically\n",
        "def correct_spelling(word):\n",
        "    suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
        "    if suggestions:\n",
        "        return suggestions[0].term  # Return the most likely correction\n",
        "    return word  # Return the original word if no correction is found\n",
        "\n",
        "# Example usage\n",
        "sentence = \"Speling erors are commn in writen text.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "corrected_sentence = \" \".join(correct_spelling(word) for word in tokens)\n",
        "\n",
        "print(\"Original Sentence:\", sentence)\n",
        "print(\"Corrected Sentence:\", corrected_sentence)\n"
      ],
      "metadata": {
        "id": "FjwX8Y_66vnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayesian method to spelling"
      ],
      "metadata": {
        "id": "rfon2o1q6vut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Load and preprocess a corpus to build a word frequency model\n",
        "def load_corpus(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        text = f.read().lower()\n",
        "    return Counter(re.findall(r'\\w+', text))\n",
        "\n",
        "# Calculate probabilities using Bayes' theorem\n",
        "def bayesian_correct(word, word_counts, alphabet=\"abcdefghijklmnopqrstuvwxyz\"):\n",
        "    def edits1(w):\n",
        "        \"\"\"All possible edits at an edit distance of 1\"\"\"\n",
        "        splits = [(w[:i], w[i:]) for i in range(len(w) + 1)]\n",
        "        deletes = [L + R[1:] for L, R in splits if R]\n",
        "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "        replaces = [L + c + R[1:] for L, R in splits if R for c in alphabet]\n",
        "        inserts = [L + c + R for L, R in splits for c in alphabet]\n",
        "        return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "    def known(words):\n",
        "        \"\"\"Filter words that exist in the dictionary\"\"\"\n",
        "        return {w for w in words if w in word_counts}\n",
        "\n",
        "    def candidates(w):\n",
        "        \"\"\"Generate possible corrections\"\"\"\n",
        "        return (known([w]) or known(edits1(w)) or [w])\n",
        "\n",
        "    def probability(w):\n",
        "        \"\"\"Calculate word probability\"\"\"\n",
        "        return word_counts[w] / sum(word_counts.values())\n",
        "\n",
        "    return max(candidates(word), key=probability)\n",
        "\n",
        "# Load a sample corpus and test the Bayesian spelling corrector\n",
        "word_counts = load_corpus('sample_corpus.txt')  # Replace with your corpus file\n",
        "misspelled_word = \"speling\"\n",
        "corrected_word = bayesian_correct(misspelled_word, word_counts)\n",
        "\n",
        "print(f\"Misspelled Word: {misspelled_word}\")\n",
        "print(f\"Corrected Word: {corrected_word}\")\n"
      ],
      "metadata": {
        "id": "4JDmf5rI6v2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minimum edit distance"
      ],
      "metadata": {
        "id": "1b5vZKgg78Od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def min_edit_distance(source, target):\n",
        "    m, n = len(source), len(target)\n",
        "    dp = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n",
        "\n",
        "    # Initialize the DP table\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i  # Cost of deletions\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j  # Cost of insertions\n",
        "\n",
        "    # Compute minimum edit distance\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if source[i - 1] == target[j - 1]:  # No cost if characters match\n",
        "                dp[i][j] = dp[i - 1][j - 1]\n",
        "            else:\n",
        "                dp[i][j] = 1 + min(\n",
        "                    dp[i - 1][j],    # Deletion\n",
        "                    dp[i][j - 1],    # Insertion\n",
        "                    dp[i - 1][j - 1] # Substitution\n",
        "                )\n",
        "\n",
        "    return dp[m][n]\n",
        "\n",
        "# Example usage\n",
        "source_word = \"intention\"\n",
        "target_word = \"execution\"\n",
        "distance = min_edit_distance(source_word, target_word)\n",
        "\n",
        "print(f\"Minimum Edit Distance between '{source_word}' and '{target_word}': {distance}\")\n"
      ],
      "metadata": {
        "id": "V7s7uVIU78XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayesian method of pronunciation variation"
      ],
      "metadata": {
        "id": "0vI72JVm78jV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import cmudict\n",
        "from collections import Counter\n",
        "\n",
        "# Load the CMU Pronouncing Dictionary\n",
        "nltk.download('cmudict')\n",
        "pronouncing_dict = cmudict.dict()\n",
        "\n",
        "# Example phonetic variation probabilities (simplified for demonstration)\n",
        "phonetic_variation_prob = {\n",
        "    \"AE\": {\"AA\": 0.2, \"AE\": 0.8},\n",
        "    \"T\": {\"D\": 0.1, \"T\": 0.9},\n",
        "}\n",
        "\n",
        "# Bayesian method for pronunciation variation\n",
        "def bayesian_pronunciation(word):\n",
        "    if word.lower() not in pronouncing_dict:\n",
        "        return f\"No pronunciation data available for '{word}'\"\n",
        "\n",
        "    pronunciations = pronouncing_dict[word.lower()]  # All known pronunciations\n",
        "    probabilities = []\n",
        "\n",
        "    for pronunciation in pronunciations:\n",
        "        prob = 1.0\n",
        "        for phoneme in pronunciation:\n",
        "            if phoneme in phonetic_variation_prob:\n",
        "                prob *= phonetic_variation_prob[phoneme].get(phoneme, 0.1)\n",
        "            else:\n",
        "                prob *= 1.0  # Assume full probability for unchanged phonemes\n",
        "        probabilities.append((tuple(pronunciation), prob))\n",
        "\n",
        "    # Select the most likely pronunciation\n",
        "    best_pronunciation = max(probabilities, key=lambda x: x[1])\n",
        "    return best_pronunciation\n",
        "\n",
        "# Example usage\n",
        "word = \"data\"\n",
        "result = bayesian_pronunciation(word)\n",
        "print(f\"Most likely pronunciation for '{word}': {result[0]} (Probability: {result[1]:.2f})\")\n"
      ],
      "metadata": {
        "id": "EZKmwJ1m78qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Viterbi algorithm"
      ],
      "metadata": {
        "id": "2ZH4wq5478y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# Load a tagged dataset\n",
        "nltk.download('treebank')\n",
        "tagged_sents = treebank.tagged_sents()[:3000]\n",
        "\n",
        "# Train HMM parameters (simplified)\n",
        "def train_hmm(tagged_sents):\n",
        "    tags = []\n",
        "    words = []\n",
        "    for sent in tagged_sents:\n",
        "        tags.append(\"START\")  # Add a START tag for the beginning\n",
        "        for word, tag in sent:\n",
        "            words.append(word)\n",
        "            tags.append(tag)\n",
        "        tags.append(\"END\")  # Add an END tag at the end\n",
        "\n",
        "    tag_freq = nltk.ConditionalFreqDist(nltk.bigrams(tags))\n",
        "    word_tag_freq = nltk.ConditionalFreqDist(((tag, word) for sent in tagged_sents for word, tag in sent))\n",
        "    return tag_freq, word_tag_freq\n",
        "\n",
        "tag_freq, word_tag_freq = train_hmm(tagged_sents)\n",
        "\n",
        "# Viterbi Algorithm\n",
        "def viterbi(sentence, tag_freq, word_tag_freq):\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    states = set(tag for tag, _ in tag_freq.items())\n",
        "    V = [{}]\n",
        "    path = {}\n",
        "\n",
        "    # Initialize base cases (t = 0)\n",
        "    for tag in states:\n",
        "        V[0][tag] = tag_freq[\"START\"].freq(tag) * word_tag_freq[tag].freq(words[0])\n",
        "        path[tag] = [tag]\n",
        "\n",
        "    # Run Viterbi for t > 0\n",
        "    for t in range(1, len(words)):\n",
        "        V.append({})\n",
        "        new_path = {}\n",
        "        for tag in states:\n",
        "            (prob, state) = max(\n",
        "                (V[t-1][y0] * tag_freq[y0].freq(tag) * word_tag_freq[tag].freq(words[t]), y0)\n",
        "                for y0 in states if V[t-1][y0] > 0\n",
        "            )\n",
        "            V[t][tag] = prob\n",
        "            new_path[tag] = path[state] + [tag]\n",
        "        path = new_path\n",
        "\n",
        "    # Find the most probable state sequence\n",
        "    (prob, state) = max((V[-1][y], y) for y in states)\n",
        "    return prob, path[state]\n",
        "\n",
        "# Example usage\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "probability, best_path = viterbi(sentence, tag_freq, word_tag_freq)\n",
        "print(f\"Most likely tags: {best_path}\")\n",
        "print(f\"Sequence Probability: {probability}\")\n"
      ],
      "metadata": {
        "id": "5a_ucfCM786-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Speech recognition."
      ],
      "metadata": {
        "id": "xfoE94Y679CW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr\n",
        "\n",
        "# Initialize recognizer\n",
        "recognizer = sr.Recognizer()\n",
        "\n",
        "# Function to recognize speech from an audio file\n",
        "def recognize_speech_from_audio(audio_file):\n",
        "    with sr.AudioFile(audio_file) as source:\n",
        "        audio_data = recognizer.record(source)\n",
        "    try:\n",
        "        # Use Google Web Speech API for speech recognition\n",
        "        recognized_text = recognizer.recognize_google(audio_data)\n",
        "        return recognized_text\n",
        "    except sr.UnknownValueError:\n",
        "        return \"Sorry, I could not understand the audio.\"\n",
        "    except sr.RequestError:\n",
        "        return \"Could not request results from the speech recognition service.\"\n",
        "\n",
        "# Example usage\n",
        "audio_file = \"example_audio.wav\"  # Replace with the path to your audio file\n",
        "recognized_text = recognize_speech_from_audio(audio_file)\n",
        "\n",
        "print(f\"Recognized Speech: {recognized_text}\")\n"
      ],
      "metadata": {
        "id": "WTTFlyln79J3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}